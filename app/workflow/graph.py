"""
LangGraphå·¥ä½œæµæ¨¡å—
ä»åŸmain.pyä¸­å‰¥ç¦»å‡ºæ¥çš„æ ¸å¿ƒAgentå·¥ä½œæµé€»è¾‘
"""
import os
import json
from typing import Literal
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, MessagesState, END
from langgraph.prebuilt import ToolNode
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage

from app.config import LANGGRAPH_RECURSION_LIMIT
from app.state import state_manager


def load_system_prompt() -> str:
    """åŠ è½½æ¿€æ´»çš„å…ƒæç¤ºè¯"""
    try:
        prompts_file = "/app/data/system_prompts.json"
        if os.path.exists(prompts_file):
            with open(prompts_file, 'r', encoding='utf-8') as f:
                prompts = json.load(f)
                for prompt in prompts:
                    if prompt.get("is_active", False):
                        return prompt["prompt"]
        
        # é»˜è®¤æç¤ºè¯
        return """ä½ æ˜¯ Agent 6ï¼Œä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ AI Agentï¼Œæ‹¥æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š

1. **ç½‘ç»œæœç´¢å’ŒæŠ“å–**ï¼šä½¿ç”¨ web_search æœç´¢ä¿¡æ¯ï¼Œä½¿ç”¨ web_scraper æŠ“å–ç½‘é¡µå†…å®¹
2. **æµè§ˆå™¨è‡ªåŠ¨åŒ–**ï¼šä½¿ç”¨ browser_automation è¿›è¡Œå¤æ‚çš„ç½‘é¡µäº¤äº’
3. **ä»£ç æ‰§è¡Œ**ï¼šä½¿ç”¨ code_executor åœ¨å®‰å…¨æ²™ç›’ä¸­æ‰§è¡Œ Python/JavaScript/Bash ä»£ç 
4. **æ–‡ä»¶æ“ä½œ**ï¼šä½¿ç”¨ file_operations è¯»å†™æ–‡ä»¶
5. **å›¾åƒå¤„ç†**ï¼šä½¿ç”¨ image_ocr è¯†åˆ«å›¾ç‰‡æ–‡å­—ï¼Œä½¿ç”¨ image_analysis åˆ†æå›¾åƒ
6. **è¯­éŸ³å¤„ç†**ï¼šä½¿ç”¨ speech_recognition_tool è½¬å½•éŸ³é¢‘
7. **æ•°æ®åˆ†æ**ï¼šä½¿ç”¨ data_analysis å¤„ç†å’Œå¯è§†åŒ–æ•°æ®
8. **è¿œç¨‹æ“ä½œ**ï¼šä½¿ç”¨ ssh_tool æ‰§è¡Œè¿œç¨‹å‘½ä»¤ï¼Œä½¿ç”¨ git_tool ç®¡ç†ä»£ç ä»“åº“
9. **API è°ƒç”¨**ï¼šä½¿ç”¨ universal_api è°ƒç”¨ä»»æ„ RESTful API
10. **é€šè®¯**ï¼šä½¿ç”¨ telegram_tool å‘é€ Telegram æ¶ˆæ¯
11. **RPAè‡ªåŠ¨åŒ–**ï¼šä½¿ç”¨ rpa_tool è¿›è¡Œå¤æ‚çš„è‡ªåŠ¨åŒ–æµç¨‹
12. **æ–‡ä»¶åŒæ­¥**ï¼šä½¿ç”¨ file_sync_tool åŒæ­¥æ–‡ä»¶åˆ°D5èˆªæ¯

**å·¥ä½œåŸåˆ™**ï¼š
- æ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼Œä¸»åŠ¨é€‰æ‹©åˆé€‚çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡
- å¦‚æœä¸€ä¸ªå·¥å…·ä¸å¤Ÿï¼Œå¯ä»¥è¿ç»­è°ƒç”¨å¤šä¸ªå·¥å…·
- å§‹ç»ˆå‘ç”¨æˆ·è§£é‡Šä½ åœ¨åšä»€ä¹ˆä»¥åŠä¸ºä»€ä¹ˆè¿™æ ·åš
- å¦‚æœé‡åˆ°é”™è¯¯ï¼Œå°è¯•å…¶ä»–æ–¹æ³•æˆ–å‘ç”¨æˆ·å¯»æ±‚å¸®åŠ©

ç°åœ¨ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„è¯·æ±‚ï¼Œå……åˆ†åˆ©ç”¨ä½ çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡ï¼"""
    except Exception as e:
        print(f"Error loading system prompt: {e}")
        return "ä½ æ˜¯ Agent 6ï¼Œä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ AI åŠ©æ‰‹ã€‚"


def agent_node(state: MessagesState, config: dict) -> MessagesState:
    """Agent èŠ‚ç‚¹ï¼šLLM æ¨ç†å¹¶å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·"""
    messages = state["messages"]
    
    # æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚æœç¬¬ä¸€æ¡æ¶ˆæ¯ä¸æ˜¯ SystemMessageï¼‰
    if not messages or not isinstance(messages[0], SystemMessage):
        system_prompt = load_system_prompt()
        messages = [SystemMessage(content=system_prompt)] + messages
    
    # ä»å…¨å±€çŠ¶æ€è·å–llm_with_tools
    llm_with_tools = state_manager.app_state.get("llm_with_tools")
    if not llm_with_tools:
        raise RuntimeError("LLMæœªåˆå§‹åŒ–")
    
    # è°ƒç”¨ LLMï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    try:
        response = llm_with_tools.with_retry(stop_after_attempt=3).invoke(messages, config=config)
        return {"messages": [response]}
    except Exception as e:
        error_message = f"LLM è°ƒç”¨å¤±è´¥: {e}"
        print(f"ERROR: {error_message}")
        return {"messages": [AIMessage(content=error_message)]}


def should_continue(state: MessagesState) -> Literal["tools", END]:
    """æ¡ä»¶è¾¹ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­è°ƒç”¨å·¥å…·"""
    messages = state["messages"]
    last_message = messages[-1]
    
    # å¦‚æœ LLM è¿”å›äº† tool_callsï¼Œåˆ™ç»§ç»­è°ƒç”¨å·¥å…·
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    
    # å¦åˆ™ç»“æŸ
    return END


def tool_node_with_error_handling(state: MessagesState) -> MessagesState:
    """å·¥å…·èŠ‚ç‚¹ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰"""
    messages = state["messages"]
    last_message = messages[-1]
    
    tool_invocations = []
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        tool_invocations = last_message.tool_calls
    
    if not tool_invocations:
        return {"messages": [AIMessage(content="æ²¡æœ‰å¯ç”¨çš„å·¥å…·è°ƒç”¨")]}
    
    # ä»å…¨å±€çŠ¶æ€è·å–tools
    tools = state_manager.app_state.get("tools", [])
    tool_node = ToolNode(tools)
    
    try:
        # è°ƒç”¨å·¥å…·èŠ‚ç‚¹
        return tool_node.invoke(state)
    except Exception as e:
        error_message = f"å·¥å…·è°ƒç”¨å¤±è´¥: {e}"
        print(f"ERROR: {error_message}")
        # è¿”å›é”™è¯¯ä¿¡æ¯ç»™ LLMï¼Œè®©å®ƒå†³å®šä¸‹ä¸€æ­¥
        return {"messages": [ToolMessage(content=error_message, tool_call_id=tool_invocations[0]["id"])]}


def create_app_graph():
    """åˆ›å»ºå¹¶ç¼–è¯‘LangGraphå·¥ä½œæµ"""
    print("ğŸ”§ æ­£åœ¨åˆ›å»ºLangGraphå·¥ä½œæµ...")
    
    # åˆ›å»ºå·¥ä½œæµ
    workflow = StateGraph(MessagesState)
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", tool_node_with_error_handling)
    workflow.set_entry_point("agent")
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            END: END
        }
    )
    workflow.add_edge("tools", "agent")
    
    # åˆ›å»ºcheckpointer
    checkpointer = MemorySaver()
    
    # ç¼–è¯‘å·¥ä½œæµ
    app_graph = workflow.compile(checkpointer=checkpointer)
    
    print("âœ… LangGraphå·¥ä½œæµåˆ›å»ºå®Œæˆ")
    return app_graph
