"""
M3 Agent System v2.5 - Main Application
å®Œæ•´çš„ Agent å·¥ä½œæµï¼Œæ”¯æŒå·¥å…·è°ƒç”¨å’Œ OpenAI å…¼å®¹æ¥å£
"""

from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import List, Optional, Dict, Any, Literal
from config.settings import settings
from config.logging_config import main_logger as logger
from app.tools import *
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, MessagesState, END
from langgraph.prebuilt import ToolNode

from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage
import httpx
import json
import time
import asyncio
import os

# ============================================
# FastAPI Application Setup
# ============================================

app = FastAPI(
    title="M3 Agent System",
    version="3.0.0",
    description="å®Œæ•´çš„ AI Agent ç³»ç»Ÿï¼Œæ”¯æŒå·¥å…·è°ƒç”¨ã€RPAè‡ªåŠ¨åŒ–å’Œå¤šè½®å¯¹è¯"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†ŒFleet APIè·¯ç”±ï¼ˆç”¨äºä¸D5ç®¡ç†èˆªæ¯å’ŒTemporalè°ƒåº¦ç³»ç»Ÿå¯¹æ¥ï¼‰
from app.api.fleet_api import router as fleet_router
app.include_router(fleet_router)

# æ³¨å†ŒLangGraph APIè·¯ç”±ï¼ˆç”¨äºassistant-uiç­‰å®¢æˆ·ç«¯ï¼‰
from app.api.langgraph_adapter import router as langgraph_router
app.include_router(langgraph_router)

# ============================================
# Initialize LLM and Tools
# ============================================

# Initialize LLM
llm = ChatOpenAI(
    base_url=settings.LLM_BASE_URL,
    model=settings.LLM_MODEL,
    temperature=settings.LLM_TEMPERATURE,
    max_tokens=settings.LLM_MAX_TOKENS,
    api_key="not-needed"
)

# Initialize all 15 tools (v2.9)
tools = [
    WebSearchTool(),
    WebScraperTool(),
    CodeExecutorTool(),
    FileOperationsTool(),
    ImageOCRTool(),
    ImageAnalysisTool(),
    SSHTool(),
    GitTool(),
    DataAnalysisTool(),
    BrowserAutomationTool(),
    UniversalAPITool(),
    TelegramTool(),
    SpeechRecognitionTool(),
    RPATool(),           # v2.8æ–°å¢ï¼šè·¨å¹³å°RPAè‡ªåŠ¨åŒ–
    FileSyncTool(),      # v2.8æ–°å¢ï¼šå®¹å™¨-å®¿ä¸»æœºæ–‡ä»¶åŒæ­¥
]

# Bind tools to LLM
llm_with_tools = llm.bind_tools(tools)

# ============================================
# Agent Workflow (LangGraph)
# ============================================

def load_system_prompt() -> str:
    """åŠ è½½æ¿€æ´»çš„å…ƒæç¤ºè¯"""
    try:
        prompts_file = "/app/data/system_prompts.json"
        if os.path.exists(prompts_file):
            with open(prompts_file, 'r', encoding='utf-8') as f:
                prompts = json.load(f)
                for prompt in prompts:
                    if prompt.get("is_active", False):
                        return prompt["prompt"]
        # é»˜è®¤æç¤ºè¯
        return """ä½ æ˜¯ M3 Agentï¼Œä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ AI åŠ©æ‰‹ï¼Œæ‹¥æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š

1. **ç½‘ç»œæœç´¢å’ŒæŠ“å–**ï¼šä½¿ç”¨ web_search æœç´¢ä¿¡æ¯ï¼Œä½¿ç”¨ web_scraper æŠ“å–ç½‘é¡µå†…å®¹
2. **æµè§ˆå™¨è‡ªåŠ¨åŒ–**ï¼šä½¿ç”¨ browser_automation è¿›è¡Œå¤æ‚çš„ç½‘é¡µäº¤äº’
3. **ä»£ç æ‰§è¡Œ**ï¼šä½¿ç”¨ code_executor åœ¨å®‰å…¨æ²™ç›’ä¸­æ‰§è¡Œ Python/JavaScript/Bash ä»£ç 
4. **æ–‡ä»¶æ“ä½œ**ï¼šä½¿ç”¨ file_operations è¯»å†™æ–‡ä»¶
5. **å›¾åƒå¤„ç†**ï¼šä½¿ç”¨ image_ocr è¯†åˆ«å›¾ç‰‡æ–‡å­—ï¼Œä½¿ç”¨ image_analysis åˆ†æå›¾åƒ
6. **æ•°æ®åˆ†æ**ï¼šä½¿ç”¨ data_analysis å¤„ç†å’Œå¯è§†åŒ–æ•°æ®
7. **è¿œç¨‹æ“ä½œ**ï¼šä½¿ç”¨ ssh_tool æ‰§è¡Œè¿œç¨‹å‘½ä»¤ï¼Œä½¿ç”¨ git_tool ç®¡ç†ä»£ç ä»“åº“
8. **API è°ƒç”¨**ï¼šä½¿ç”¨ universal_api è°ƒç”¨ä»»æ„ RESTful API
9. **é€šè®¯**ï¼šä½¿ç”¨ telegram_tool å‘é€ Telegram æ¶ˆæ¯

**å·¥ä½œåŸåˆ™**ï¼š
- æ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼Œä¸»åŠ¨é€‰æ‹©åˆé€‚çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡
- å¦‚æœä¸€ä¸ªå·¥å…·ä¸å¤Ÿï¼Œå¯ä»¥è¿ç»­è°ƒç”¨å¤šä¸ªå·¥å…·
- å§‹ç»ˆå‘ç”¨æˆ·è§£é‡Šä½ åœ¨åšä»€ä¹ˆä»¥åŠä¸ºä»€ä¹ˆè¿™æ ·åš
- å¦‚æœé‡åˆ°é”™è¯¯ï¼Œå°è¯•å…¶ä»–æ–¹æ³•æˆ–å‘ç”¨æˆ·å¯»æ±‚å¸®åŠ©

ç°åœ¨ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„è¯·æ±‚ï¼Œå……åˆ†åˆ©ç”¨ä½ çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡ï¼"""
    except Exception as e:
        logger.error(f"Error loading system prompt: {e}")
        return "ä½ æ˜¯ M3 Agentï¼Œä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ AI åŠ©æ‰‹ã€‚"

def agent_node(state: MessagesState, config: dict) -> MessagesState:
    """Agent èŠ‚ç‚¹ï¼šLLM æ¨ç†å¹¶å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·"""
    messages = state["messages"]
    
    # æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚æœç¬¬ä¸€æ¡æ¶ˆæ¯ä¸æ˜¯ SystemMessageï¼‰
    if not messages or not isinstance(messages[0], SystemMessage):
        system_prompt = load_system_prompt()
        messages = [SystemMessage(content=system_prompt)] + messages
    
    # è°ƒç”¨ LLMï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    try:
        response = llm_with_tools.with_retry(stop_after_attempt=3).invoke(messages, config=config)
        return {"messages": [response]}
    except Exception as e:
        error_message = f"LLM è°ƒç”¨å¤±è´¥: {e}"
        logger.error(error_message)
        return {"messages": [AIMessage(content=error_message)]}

def should_continue(state: MessagesState) -> Literal["tools", END]:
    """æ¡ä»¶è¾¹ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­è°ƒç”¨å·¥å…·"""
    messages = state["messages"]
    last_message = messages[-1]
    
    # å¦‚æœ LLM è¿”å›äº† tool_callsï¼Œåˆ™ç»§ç»­è°ƒç”¨å·¥å…·
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    
    # å¦åˆ™ç»“æŸ
    return END

# åˆ›å»ºå·¥å…·èŠ‚ç‚¹
def tool_node_with_error_handling(state: MessagesState) -> MessagesState:
    """å·¥å…·èŠ‚ç‚¹ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰"""
    messages = state["messages"]
    last_message = messages[-1]
    
    tool_invocations = []
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        tool_invocations = last_message.tool_calls
    
    if not tool_invocations:
        return {"messages": [AIMessage(content="æ²¡æœ‰å¯ç”¨çš„å·¥å…·è°ƒç”¨")]}
    
    tool_node = ToolNode(tools)
    try:
        # è°ƒç”¨å·¥å…·èŠ‚ç‚¹
        return tool_node.invoke(state)
    except Exception as e:
        error_message = f"å·¥å…·è°ƒç”¨å¤±è´¥: {e}"
        logger.error(error_message)
        # è¿”å›é”™è¯¯ä¿¡æ¯ç»™ LLMï¼Œè®©å®ƒå†³å®šä¸‹ä¸€æ­¥
        return {"messages": [ToolMessage(content=error_message, tool_call_id=tool_invocations[0]["id"])]}

tool_node = tool_node_with_error_handling

# æ„å»º LangGraph å·¥ä½œæµ
workflow = StateGraph(MessagesState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("agent", agent_node)
workflow.add_node("tools", tool_node_with_error_handling)

# è®¾ç½®å…¥å£ç‚¹
workflow.set_entry_point("agent")

# æ·»åŠ æ¡ä»¶è¾¹
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "tools": "tools",
        END: END
    }
)

# å·¥å…·æ‰§è¡Œåï¼Œå›åˆ° agent èŠ‚ç‚¹
workflow.add_edge("tools", "agent")

# é…ç½®å†…å­˜æŒä¹…åŒ–ï¼ˆä½¿ç”¨ MemorySaverï¼‰
# v2.5: å·²ç§»é™¤PostgreSQLä¾èµ–ï¼Œä½¿ç”¨å†…å­˜checkpointer
# æœªæ¥å°†é€šè¿‡ D5 è®°å¿†èˆªæ¯å®ç°é›†ä¸­å¼è®°å¿†ç®¡ç†
checkpointer = MemorySaver()
app_graph = workflow.compile(checkpointer=checkpointer)
print("âœ“ LangGraph workflow compiled with MemorySaver (in-memory checkpointer)")

# ============================================
# Pydantic Models
# ============================================

class ChatRequest(BaseModel):
    message: str
    thread_id: str = "default"

class ChatResponse(BaseModel):
    response: str
    thread_id: str
    tool_calls: Optional[List[Dict[str, Any]]] = None

class OpenAIMessage(BaseModel):
    role: str
    content: Optional[str] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None

class OpenAIChatRequest(BaseModel):
    model: str
    messages: List[OpenAIMessage]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = None
    stream: Optional[bool] = False
    tools: Optional[List[Dict[str, Any]]] = None
    tool_choice: Optional[str] = "auto"

class OpenAIModel(BaseModel):
    id: str
    object: str = "model"
    created: int = int(time.time())
    owned_by: str = "m3-agent"

class OpenAIModelsResponse(BaseModel):
    object: str = "list"
    data: List[OpenAIModel]

# ============================================
# Native Agent API Endpoints
# ============================================

@app.get("/")
async def root():
    return {
        "status": "M3 Agent System v2.2.0 Running",
        "tools": len(tools),
        "features": ["Agent Workflow", "Tool Calling", "OpenAI Compatible"]
    }

@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "llm_model": settings.LLM_MODEL,
        "tools_count": len(tools)
    }

@app.post("/api/agent/chat", response_model=ChatResponse)
async def agent_chat(request: ChatRequest):
    """åŸç”Ÿ Agent æ¥å£ï¼šæ”¯æŒå®Œæ•´çš„å·¥å…·è°ƒç”¨å·¥ä½œæµ"""
    try:
        # æ„å»ºè¾“å…¥æ¶ˆæ¯
        input_messages = [HumanMessage(content=request.message)]
        
        # è°ƒç”¨ Agent å·¥ä½œæµ
        config = {"configurable": {"thread_id": request.thread_id}}
        result = app_graph.invoke(
            {"messages": input_messages},
            config=config
        )
        
        # æå–æœ€ç»ˆå“åº”
        messages = result["messages"]
        last_message = messages[-1]
        
        # æå–å·¥å…·è°ƒç”¨ä¿¡æ¯
        tool_calls_info = []
        for msg in messages:
            if hasattr(msg, "tool_calls") and msg.tool_calls:
                for tc in msg.tool_calls:
                    tool_calls_info.append({
                        "tool": tc["name"],
                        "input": tc["args"]
                    })
        
        return ChatResponse(
            response=last_message.content if hasattr(last_message, "content") else str(last_message),
            thread_id=request.thread_id,
            tool_calls=tool_calls_info if tool_calls_info else None
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Agent execution failed: {str(e)}")

@app.get("/api/tools")
async def list_tools():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨å·¥å…·"""
    return {
        "tools": [
            {
                "name": tool.name,
                "description": tool.description
            }
            for tool in tools
        ]
    }

# ============================================
# OpenAI Compatible API Endpoints
# ============================================

@app.get("/v1/models")
async def list_models():
    """OpenAI å…¼å®¹ï¼šåˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    try:
        # å°è¯•ä» LM Studio è·å–æ¨¡å‹åˆ—è¡¨
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(f"{settings.LLM_BASE_URL}/models")
            if response.status_code == 200:
                return response.json()
    except Exception:
        pass
    
    # å›é€€ï¼šè¿”å›é…ç½®çš„æ¨¡å‹
    return OpenAIModelsResponse(
        data=[OpenAIModel(id=settings.LLM_MODEL)]
    ).dict()

@app.post("/v1/chat/completions")
async def chat_completions(request: OpenAIChatRequest):
    """OpenAI å…¼å®¹ï¼šèŠå¤©è¡¥å…¨æ¥å£ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰"""
    try:
        # è½¬æ¢ OpenAI æ ¼å¼çš„æ¶ˆæ¯ä¸º LangChain æ ¼å¼
        langchain_messages = []
        for msg in request.messages:
            if msg.role == "system":
                langchain_messages.append(SystemMessage(content=msg.content))
            elif msg.role == "user":
                langchain_messages.append(HumanMessage(content=msg.content))
            elif msg.role == "assistant":
                langchain_messages.append(AIMessage(content=msg.content or ""))
        
        # è°ƒç”¨ Agent å·¥ä½œæµ
        config = {"configurable": {"thread_id": f"openai_{int(time.time())}"}}
        result = app_graph.invoke(
            {"messages": langchain_messages},
            config=config
        )
        
        # æå–æœ€ç»ˆå“åº”
        messages = result["messages"]
        last_message = messages[-1]
        
        # æ„å»º OpenAI æ ¼å¼çš„å“åº”
        response_message = {
            "role": "assistant",
            "content": last_message.content if hasattr(last_message, "content") else str(last_message)
        }
        
        # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ·»åŠ  tool_calls
        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            response_message["tool_calls"] = [
                {
                    "id": f"call_{i}",
                    "type": "function",
                    "function": {
                        "name": tc["name"],
                        "arguments": json.dumps(tc["args"])
                    }
                }
                for i, tc in enumerate(last_message.tool_calls)
            ]
        
        # è¿”å› OpenAI æ ¼å¼
        return {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": request.model,
            "choices": [
                {
                    "index": 0,
                    "message": response_message,
                    "finish_reason": "tool_calls" if "tool_calls" in response_message else "stop"
                }
            ],
            "usage": {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0
            }
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chat completion failed: {str(e)}")

@app.post("/v1/completions")
async def completions(request: Dict[str, Any]):
    """OpenAI å…¼å®¹ï¼šæ–‡æœ¬è¡¥å…¨æ¥å£ï¼ˆä»£ç†åˆ° LM Studioï¼‰"""
    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                f"{settings.LLM_BASE_URL}/completions",
                json=request,
                headers={"Content-Type": "application/json"}
            )
            return response.json()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Completion failed: {str(e)}")

@app.post("/v1/embeddings")
async def embeddings(request: Dict[str, Any]):
    """OpenAI å…¼å®¹ï¼šæ–‡æœ¬åµŒå…¥æ¥å£ï¼ˆä»£ç†åˆ° LM Studioï¼‰"""
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{settings.LLM_BASE_URL}/embeddings",
                json=request,
                headers={"Content-Type": "application/json"}
            )
            return response.json()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Embeddings failed: {str(e)}")

# ============================================
# WebSocket Chat Room (é¢„ç•™æ¥å£)
# ============================================

class ConnectionManager:
    """WebSocket è¿æ¥ç®¡ç†å™¨ï¼ˆä¸ºæœªæ¥çš„èˆ°é˜ŸèŠå¤©å®¤é¢„ç•™ï¼‰"""
    def __init__(self):
        self.active_connections: List[WebSocket] = []
    
    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)
    
    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)
    
    async def broadcast(self, message: str):
        for connection in self.active_connections:
            try:
                await connection.send_text(message)
            except:
                pass

manager = ConnectionManager()

@app.websocket("/ws/chat")
async def websocket_chat(websocket: WebSocket):
    """WebSocket èŠå¤©æ¥å£ï¼ˆé¢„ç•™ï¼‰"""
    await manager.connect(websocket)
    try:
        while True:
            data = await websocket.receive_text()
            # TODO: æœªæ¥åœ¨è¿™é‡Œé›†æˆ Agent å·¥ä½œæµ
            # ç›®å‰åªæ˜¯ç®€å•å›æ˜¾
            await manager.broadcast(f"Echo: {data}")
    except WebSocketDisconnect:
        manager.disconnect(websocket)

@app.get("/chat-room", response_class=HTMLResponse)
async def chat_room_placeholder():
    """èŠå¤©å®¤å ä½é¡µé¢ï¼ˆæœªæ¥æ›¿æ¢ä¸ºå®Œæ•´çš„å‰ç«¯ï¼‰"""
    return """
    <!DOCTYPE html>
    <html>
    <head>
        <title>M3 Agent èŠå¤©å®¤ï¼ˆå¼€å‘ä¸­ï¼‰</title>
        <style>
            body { font-family: Arial, sans-serif; max-width: 800px; margin: 50px auto; padding: 20px; }
            h1 { color: #667eea; }
            .note { background: #f0f0f0; padding: 15px; border-radius: 8px; }
        </style>
    </head>
    <body>
        <h1>ğŸš€ M3 Agent èˆ°é˜ŸèŠå¤©å®¤</h1>
        <div class="note">
            <p><strong>çŠ¶æ€ï¼š</strong>å¼€å‘ä¸­</p>
            <p><strong>WebSocket ç«¯ç‚¹ï¼š</strong><code>ws://localhost:8001/ws/chat</code></p>
            <p><strong>è®¡åˆ’åŠŸèƒ½ï¼š</strong></p>
            <ul>
                <li>å¤šç”¨æˆ·å®æ—¶èŠå¤©</li>
                <li>Agent å·¥å…·è°ƒç”¨å¯è§†åŒ–</li>
                <li>å¯¹è¯å†å²å›æ”¾</li>
                <li>å¤šæ¨¡æ€æ¶ˆæ¯æ”¯æŒï¼ˆæ–‡æœ¬ã€å›¾ç‰‡ã€ä»£ç ï¼‰</li>
            </ul>
        </div>
    </body>
    </html>
    """

# ============================================
# Mount Admin Panel
# ============================================

# æ³¨æ„ï¼šadmin_app å°†åœ¨å•ç‹¬çš„è¿›ç¨‹ä¸­è¿è¡Œï¼ˆç«¯å£ 8002ï¼‰
# è¿™é‡Œä¸éœ€è¦ mountï¼Œå› ä¸ºå®ƒä»¬æ˜¯ç‹¬ç«‹çš„æœåŠ¡

# ============================================
# Memory Sync Startup
# ============================================

from app.memory.memory_sync import start_memory_sync, stop_memory_sync
import atexit

# å¯åŠ¨è®°å¿†åŒæ­¥
start_memory_sync()
logger.info("âœ“ Memory sync worker started")

# æ³¨å†Œå…³é—­é’©å­
atexit.register(stop_memory_sync)

# ============================================
# Main Entry Point
# ============================================

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(
        app,
        host=settings.API_HOST,
        port=settings.API_PORT,
        log_level="info"
    )
