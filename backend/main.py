"""FastAPI Backend for M3 Agent System
A6 System v6.2.0 - Main Application
é‡å¤§æ€§èƒ½ä¼˜åŒ–ï¼šå…¨å±€æµè§ˆå™¨æ±  + æ¨¡å‹é¢„åŠ è½½
å®Œæ•´çš„ Agent å·¥ä½œæµï¼Œæ”¯æŒå·¥å…·è°ƒç”¨å’Œ OpenAI å…¼å®¹æ¥å£

v5.2 Critical Fix:
- Migrated browser_pool from sync_playwright() to async_playwright()
- Created sync/async bridge (browser_sync_wrapper) for tool compatibility
- Fixed "Playwright Sync API inside asyncio loop" error
- All Playwright tools updated to use async browser pool

v5.1 Bug Fixes:
- Fixed event loop conflict (browser pool initialization moved to startup event)
- Fixed frontend nginx config (removed backend proxy)

v5.0 Performance Improvements:
- Browser Pool: 90% faster Playwright operations (5-10s â†’ 0.5-1s)
- Model Pre-loading: 60% faster first-time model usage
- Memory optimization: Shared browser instances across tools

v5.7 Tool Pool:
- Global tool pool for pre-loading heavy resources (OCR, Docker, etc.)
- 10-20x faster first-time tool calls (OCR: 10s â†’ 0.5s)
- Memory usage: ~1.3GB (0.7% of 192GB M3 memory)

v5.6 Critical Fixes:
- Fixed event loop conflict with nest_asyncio (browser pool now works)
- Fixed performance monitoring task lifecycle (ä¿æŒä»»åŠ¡å¼•ç”¨)
- Added API performance tracking middleware
- Updated version to v5.6
"""

# v5.7.1: Tool pool for pre-loading heavy resources
# v5.7.1: Browser pool uses thread pool (no need for nest_asyncio)
# Removed nest_asyncio to preserve uvloop performance

from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, HTMLResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from typing import List, Optional, Dict, Any, Literal
from config.settings import settings
from config.logging_config import main_logger as logger
from app.tools import *
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, MessagesState, END
from langgraph.prebuilt import ToolNode

from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, SystemMessage
import httpx
import json
import time
import asyncio
import os

# WebSocketç®¡ç†å™¨
from app.websocket_manager import manager as ws_manager

# ============================================
# Global Application State (v6.4)
# ============================================
# ä½¿ç”¨å•ä¾‹StateManager,ç¡®ä¿è·¨æ¨¡å—çŠ¶æ€å…±äº«
from app.core.state_manager import StateManager

state_mgr = StateManager()

# å‘åå…¼å®¹:ä¿ç•™app_stateå˜é‡å(ä½†å®é™…ä½¿ç”¨state_mgr)
# è¿™æ ·å¯ä»¥å‡å°‘ä»£ç ä¿®æ”¹é‡
app_state = state_mgr._state  # ç›´æ¥å¼•ç”¨å†…éƒ¨å­—å…¸

# ============================================
# FastAPI Application Setup
# ============================================

app = FastAPI(
    title="Agent System",
    version="5.9.0",
    description="M3 Agent v5.9.0 - Background Tasks Manager: Periodic health checks (tool pool + performance test + API check). åå°ä»»åŠ¡ç®¡ç†å™¨ï¼šå®šæœŸå¥åº·æ£€æŸ¥ï¼ˆå·¥å…·æ± é¢„åŠ è½½ + æ€§èƒ½æµ‹è¯• + APIæ£€æµ‹ï¼‰ã€‚æ”¯æŒæ€ç»´é“¾+å·¥å…·é“¾ã€ä¸‰è§’èŠå¤©å®¤ã€SSEæµå¼è¾“å‡ºã€å·¥å…·è°ƒç”¨ã€RPAè‡ªåŠ¨åŒ–ã€å¤šè½®å¯¹è¯å’Œæ€§èƒ½ç›‘æ§"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# v5.6: Add API performance tracking middleware
from app.performance.performance_monitor import record_api_request
import time

@app.middleware("http")
async def performance_tracking_middleware(request: Request, call_next):
    """Track API performance for all requests"""
    start_time = time.time()
    try:
        response = await call_next(request)
        response_time = (time.time() - start_time) * 1000
        record_api_request(success=response.status_code < 400, response_time_ms=response_time)
        return response
    except Exception as e:
        response_time = (time.time() - start_time) * 1000
        record_api_request(success=False, response_time_ms=response_time)
        raise

# v5.1: Startup event to initialize browser pool, tools, and workflow
@app.on_event("startup")
async def startup_event():
    """Initialize browser pool, tools, and workflow on application startup."""
    from app.core.startup import initialize_browser_pool_and_tools
    
    # v6.3.2: Browser pool and tools will be loaded by background tasks (5 minutes after startup)
    logger.info("[v6.3.2] Browser pool and tools will be loaded in background (5 minutes delay)")
    logger.info("[v6.3.2] This avoids asyncio conflicts during startup")
    
    # Initialize app_state with empty values (will be populated by background tasks)
    app_state["browser_pool"] = None
    app_state["tools"] = []
    
    # Bind empty tools to LLM (will be updated by background tasks)
    app_state["llm_with_tools"] = llm.bind_tools(app_state["tools"])
    
    # v5.9: Start background tasks manager
    from app.core.background_tasks import background_tasks_manager
    await background_tasks_manager.start()
    
    # Compile workflow (moved from module level to avoid using None llm_with_tools)
    from langgraph.checkpoint.memory import MemorySaver
    from langgraph.graph import StateGraph, MessagesState, END
    workflow = StateGraph(MessagesState)
    workflow.add_node("agent", agent_node)
    workflow.add_node("tools", tool_node_with_error_handling)
    workflow.set_entry_point("agent")
    workflow.add_conditional_edges(
        "agent",
        should_continue,
        {
            "tools": "tools",
            END: END
        }
    )
    workflow.add_edge("tools", "agent")
    checkpointer = MemorySaver()
    app_state["app_graph"] = workflow.compile(checkpointer=checkpointer)
    
    logger.info("âœ… Startup complete: browser pool, tools, and workflow ready")

# æ³¨å†ŒFleet APIè·¯ç”±ï¼ˆç”¨äºä¸D5ç®¡ç†èˆªæ¯å’ŒTemporalè°ƒåº¦ç³»ç»Ÿå¯¹æ¥ï¼‰
from app.api.fleet_api import router as fleet_router
app.include_router(fleet_router)

# æ³¨å†ŒLangGraph APIè·¯ç”±ï¼ˆç”¨äºassistant-uiç­‰å®¢æˆ·ç«¯ï¼‰
from app.api.langgraph_adapter import router as langgraph_router
app.include_router(langgraph_router)

# v5.8: æ³¨å†ŒStreaming APIè·¯ç”±ï¼ˆæ€ç»´é“¾ + å·¥å…·é“¾ï¼‰
from app.api.streaming import router as streaming_router
app.include_router(streaming_router)

# v5.9: æ³¨å†Œç»Ÿä¸€ä¸‰è§’èŠå¤©å®¤APIè·¯ç”±ï¼ˆé»˜è®¤ç»Ÿä¸€ç•Œé¢ï¼Œä¸‰æ–¹å¯è§ï¼‰
from app.api.unified_chat_room import router as unified_chat_room_router
app.include_router(unified_chat_room_router)

# v6.5.5: æ³¨å†ŒèŠå¤©å®¤SSEæµå¼APIè·¯ç”±ï¼ˆæŒ‚è½½åˆ°8888ç«¯å£ä»¥å…±äº«çŠ¶æ€ï¼‰
from chatroom_api import router as chatroom_api_router
app.include_router(chatroom_api_router)

# ============================================
# Initialize LLM and Tools
# ============================================

# Initialize LLM
llm = ChatOpenAI(
    base_url=settings.LLM_BASE_URL,
    model=settings.LLM_MODEL,
    temperature=settings.LLM_TEMPERATURE,
    max_tokens=settings.LLM_MAX_TOKENS,
    api_key="not-needed"
)

# v5.1: Browser pool and tools will be initialized in startup event
from app.core.browser_pool import get_browser_pool
browser_pool = None
tools = []  # Will be initialized in startup event

# v6.3.2: llm_with_tools moved to app_state dictionary

# ============================================
# Agent Workflow (LangGraph)
# ============================================

def load_system_prompt() -> str:
    """åŠ è½½æ¿€æ´»çš„å…ƒæç¤ºè¯"""
    try:
        prompts_file = "/app/data/system_prompts.json"
        if os.path.exists(prompts_file):
            with open(prompts_file, 'r', encoding='utf-8') as f:
                prompts = json.load(f)
                for prompt in prompts:
                    if prompt.get("is_active", False):
                        return prompt["prompt"]
        # é»˜è®¤æç¤ºè¯ (v6.1: æ·»åŠ VLæ¨¡å‹æ”¯æŒè¯´æ˜)
        return """ä½ æ˜¯ A6 Systemï¼Œä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ AI Agentï¼Œæ‹¥æœ‰ä»¥ä¸‹èƒ½åŠ›ï¼š

1. **ç½‘ç»œæœç´¢å’ŒæŠ“å–**ï¼šä½¿ç”¨ web_search æœç´¢ä¿¡æ¯ï¼Œä½¿ç”¨ web_scraper æŠ“å–ç½‘é¡µå†…å®¹
2. **æµè§ˆå™¨è‡ªåŠ¨åŒ–**ï¼šä½¿ç”¨ browser_automation è¿›è¡Œå¤æ‚çš„ç½‘é¡µäº¤äº’
3. **ä»£ç æ‰§è¡Œ**ï¼šä½¿ç”¨ code_executor åœ¨å®‰å…¨æ²™ç›’ä¸­æ‰§è¡Œ Python/JavaScript/Bash ä»£ç 
4. **æ–‡ä»¶æ“ä½œ**ï¼šä½¿ç”¨ file_operations è¯»å†™æ–‡ä»¶
5. **å›¾åƒå¤„ç†**ï¼šä½¿ç”¨ image_ocr è¯†åˆ«å›¾ç‰‡æ–‡å­— (æ”¯æŒä¸­è‹±æ—¥éŸ©ç­‰å¤šè¯­è¨€)ï¼Œä½¿ç”¨ image_analysis åˆ†æå›¾åƒ (äººè„¸æ£€æµ‹ã€ç‰©ä½“è¯†åˆ«ç­‰)
6. **è¯­éŸ³å¤„ç†**ï¼šä½¿ç”¨ speech_recognition_tool è½¬å½•éŸ³é¢‘ (æ”¯æŒä¸­è‹±æ—¥éŸ©ç­‰å¤šè¯­è¨€)
7. **è§†é¢‘å¤„ç†**ï¼šä½¿ç”¨ video_analysis åˆ†æè§†é¢‘å†…å®¹
8. **æ•°æ®åˆ†æ**ï¼šä½¿ç”¨ data_analysis å¤„ç†å’Œå¯è§†åŒ–æ•°æ®
9. **è¿œç¨‹æ“ä½œ**ï¼šä½¿ç”¨ ssh_tool æ‰§è¡Œè¿œç¨‹å‘½ä»¤ï¼Œä½¿ç”¨ git_tool ç®¡ç†ä»£ç ä»“åº“
10. **API è°ƒç”¨**ï¼šä½¿ç”¨ universal_api è°ƒç”¨ä»»æ„ RESTful API
11. **é€šè®¯**ï¼šä½¿ç”¨ telegram_tool å‘é€ Telegram æ¶ˆæ¯
12. **RPAè‡ªåŠ¨åŒ–**ï¼šä½¿ç”¨ rpa_tool è¿›è¡Œå¤æ‚çš„è‡ªåŠ¨åŒ–æµç¨‹
13. **æ–‡ä»¶åŒæ­¥**ï¼šä½¿ç”¨ file_sync_tool åŒæ­¥æ–‡ä»¶åˆ°D5èˆªæ¯

**å·¥ä½œåŸåˆ™**ï¼š
- é»˜è®¤ä½¿ç”¨å·¥å…·å¤„ç†å¤šåª’ä½“å†…å®¹ï¼Œä»¥ç¡®ä¿ç»“æœçš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§
- å¦‚æœä½ æ˜¯å¤šæ¨¡æ€æ¨¡å‹ (VLæ¨¡å‹)ï¼Œåœ¨ç®€å•çš„å›¾ç‰‡ç†è§£ä»»åŠ¡ä¸­ (å¦‚â€œè¿™æ˜¯ä»€ä¹ˆï¼Ÿâ€)ï¼Œä½ å¯ä»¥ç›´æ¥æŸ¥çœ‹å›¾ç‰‡
- ä½†å¯¹äºéœ€è¦ç²¾ç¡®ç»“æœçš„ä»»åŠ¡ (å¦‚OCRã€äººè„¸æ£€æµ‹ã€è¯­éŸ³è½¬å½•)ï¼Œä»åº”ä½¿ç”¨ä¸“ä¸šå·¥å…·
- æ ¹æ®ç”¨æˆ·éœ€æ±‚ï¼Œä¸»åŠ¨é€‰æ‹©åˆé€‚çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡
- å¦‚æœä¸€ä¸ªå·¥å…·ä¸å¤Ÿï¼Œå¯ä»¥è¿ç»­è°ƒç”¨å¤šä¸ªå·¥å…·
- å§‹ç»ˆå‘ç”¨æˆ·è§£é‡Šä½ åœ¨åšä»€ä¹ˆä»¥åŠä¸ºä»€ä¹ˆè¿™æ ·åš
- å¦‚æœé‡åˆ°é”™è¯¯ï¼Œå°è¯•å…¶ä»–æ–¹æ³•æˆ–å‘ç”¨æˆ·å¯»æ±‚å¸®åŠ©

ç°åœ¨ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„è¯·æ±‚ï¼Œå……åˆ†åˆ©ç”¨ä½ çš„å·¥å…·æ¥å®Œæˆä»»åŠ¡ï¼"""
    except Exception as e:
        logger.error(f"Error loading system prompt: {e}")
        return "ä½ æ˜¯ M3 Agentï¼Œä¸€ä¸ªåŠŸèƒ½å¼ºå¤§çš„ AI åŠ©æ‰‹ã€‚"

def agent_node(state: MessagesState, config: dict) -> MessagesState:
    """Agent èŠ‚ç‚¹ï¼šLLM æ¨ç†å¹¶å†³å®šæ˜¯å¦è°ƒç”¨å·¥å…·"""
    messages = state["messages"]
    
    # æ·»åŠ ç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚æœç¬¬ä¸€æ¡æ¶ˆæ¯ä¸æ˜¯ SystemMessageï¼‰
    if not messages or not isinstance(messages[0], SystemMessage):
        system_prompt = load_system_prompt()
        messages = [SystemMessage(content=system_prompt)] + messages
    
    # è°ƒç”¨ LLMï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰(v6.3.2: ä½¿ç”¨app_state)
    try:
        response = app_state["llm_with_tools"].with_retry(stop_after_attempt=3).invoke(messages, config=config)
        return {"messages": [response]}
    except Exception as e:
        error_message = f"LLM è°ƒç”¨å¤±è´¥: {e}"
        logger.error(error_message)
        return {"messages": [AIMessage(content=error_message)]}

def should_continue(state: MessagesState) -> Literal["tools", END]:
    """æ¡ä»¶è¾¹ï¼šåˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­è°ƒç”¨å·¥å…·"""
    messages = state["messages"]
    last_message = messages[-1]
    
    # å¦‚æœ LLM è¿”å›äº† tool_callsï¼Œåˆ™ç»§ç»­è°ƒç”¨å·¥å…·
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tools"
    
    # å¦åˆ™ç»“æŸ
    return END

# åˆ›å»ºå·¥å…·èŠ‚ç‚¹
def tool_node_with_error_handling(state: MessagesState) -> MessagesState:
    """å·¥å…·èŠ‚ç‚¹ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰"""
    messages = state["messages"]
    last_message = messages[-1]
    
    tool_invocations = []
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        tool_invocations = last_message.tool_calls
    
    if not tool_invocations:
        return {"messages": [AIMessage(content="æ²¡æœ‰å¯ç”¨çš„å·¥å…·è°ƒç”¨")]}
    
    tool_node = ToolNode(tools)
    try:
        # è°ƒç”¨å·¥å…·èŠ‚ç‚¹
        return tool_node.invoke(state)
    except Exception as e:
        error_message = f"å·¥å…·è°ƒç”¨å¤±è´¥: {e}"
        logger.error(error_message)
        # è¿”å›é”™è¯¯ä¿¡æ¯ç»™ LLMï¼Œè®©å®ƒå†³å®šä¸‹ä¸€æ­¥
        return {"messages": [ToolMessage(content=error_message, tool_call_id=tool_invocations[0]["id"])]}

tool_node = tool_node_with_error_handling

# v6.3.2: app_graph moved to app_state dictionary
# This allows background_tasks to update it properly

# ============================================
# Pydantic Models
# ============================================

class ChatRequest(BaseModel):
    message: str
    thread_id: str = "default"
    source: str = "user"  # æ¶ˆæ¯æ¥æº: user/api/assistant

class ChatResponse(BaseModel):
    response: str
    thread_id: str
    tool_calls: Optional[List[Dict[str, Any]]] = None

class OpenAIMessage(BaseModel):
    role: str
    content: Optional[str] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None

class OpenAIChatRequest(BaseModel):
    model: str
    messages: List[OpenAIMessage]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = None
    stream: Optional[bool] = False
    tools: Optional[List[Dict[str, Any]]] = None
    tool_choice: Optional[str] = "auto"

class OpenAIModel(BaseModel):
    id: str
    object: str = "model"
    created: int = int(time.time())
    owned_by: str = "m3-agent"

class OpenAIModelsResponse(BaseModel):
    object: str = "list"
    data: List[OpenAIModel]

# ============================================
# Native Agent API Endpoints
# ============================================

@app.get("/")
async def root():
    return {
        "status": "M3 Agent System v5.9.0 Running",
        "tools": len(tools),
        "features": ["Agent Workflow", "Tool Calling", "OpenAI Compatible"]
    }

@app.get("/health")
async def health():
    # v6.5.6: ä¿®å¤ - ä» app_state è¯»å– tools
    actual_tools = app_state.get("tools", [])
    return {
        "status": "healthy",
        "llm_model": settings.LLM_MODEL,
        "tools_count": len(actual_tools)
    }

@app.post("/api/agent/chat", response_model=ChatResponse)
async def agent_chat(request: ChatRequest):
    """åŸç”Ÿ Agent æ¥å£ï¼šæ”¯æŒå®Œæ•´çš„å·¥å…·è°ƒç”¨å·¥ä½œæµ"""
    try:
        # è®°å½•ç”¨æˆ·æ¶ˆæ¯åˆ°memory_buffer
        from app.memory.memory_logger import log_dialogue
        log_dialogue(
            role="user",
            message=request.message,
            source=request.source,
            thread_id=request.thread_id,
            interface="http_api"
        )
        
        # é€šè¿‡WebSocketå¹¿æ’­ç”¨æˆ·æ¶ˆæ¯
        await ws_manager.broadcast_to_thread(
            thread_id=request.thread_id,
            message={
                "type": "new_message",
                "thread_id": request.thread_id,
                "role": "user",
                "content": request.message,
                "source": request.source,
                "timestamp": time.time()
            }
        )
        
        # æ„å»ºè¾“å…¥æ¶ˆæ¯
        input_messages = [HumanMessage(content=request.message)]
        
        # è°ƒç”¨ Agent å·¥ä½œæµ (v6.3.2: ä½¿ç”¨app_state)
        config = {"configurable": {"thread_id": request.thread_id}}
        result = app_state["app_graph"].invoke(
            {"messages": input_messages},
            config=config
        )
        
        # æå–æœ€ç»ˆå“åº”
        messages = result["messages"]
        last_message = messages[-1]
        
        # æå–å·¥å…·è°ƒç”¨ä¿¡æ¯
        tool_calls_info = []
        for msg in messages:
            if hasattr(msg, "tool_calls") and msg.tool_calls:
                for tc in msg.tool_calls:
                    tool_calls_info.append({
                        "tool": tc["name"],
                        "input": tc["args"]
                    })
        
        response_text = last_message.content if hasattr(last_message, "content") else str(last_message)
        
        # è®°å½•Assistantçš„å›å¤åˆ°memory_buffer
        log_dialogue(
            role="assistant",
            message=response_text,
            source="assistant",
            thread_id=request.thread_id,
            interface="http_api"
        )
        
        # é€šè¿‡WebSocketå¹¿æ’­æ–°æ¶ˆæ¯
        await ws_manager.broadcast_to_thread(
            thread_id=request.thread_id,
            message={
                "type": "new_message",
                "thread_id": request.thread_id,
                "role": "assistant",
                "content": response_text,
                "source": "assistant",
                "timestamp": time.time()
            }
        )
        
        return ChatResponse(
            response=response_text,
            thread_id=request.thread_id,
            tool_calls=tool_calls_info if tool_calls_info else None
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Agent execution failed: {str(e)}")

@app.post("/api/chat")
async def simple_chat(request: ChatRequest):
    """
    ç®€åŒ–çš„èŠå¤©æ¥å£ï¼ˆå…¼å®¹æ—§ç‰ˆæœ¬ï¼‰
    
    è¿™æ˜¯ /api/agent/chat çš„åˆ«åï¼Œæä¾›æ›´ç®€æ´çš„APIè·¯å¾„ã€‚
    æ”¯æŒå®Œæ•´çš„Agentå·¥ä½œæµï¼ŒåŒ…æ‹¬å·¥å…·è°ƒç”¨å’Œå¤šè½®å¯¹è¯ã€‚
    
    Args:
        request: ChatRequest with message and optional thread_id
    
    Returns:
        ChatResponse with agent response and tool call info
    """
    return await agent_chat(request)

@app.get("/api/tools")
async def list_tools():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨å·¥å…· (v6.3.2: ä½¿ç”¨app_state)"""
    return {
        "tools": [
            {
                "name": tool.name,
                "description": tool.description
            }
            for tool in app_state["tools"]
        ]
    }

@app.get("/api/threads/{thread_id}/history")
async def get_thread_history(thread_id: str, limit: int = 100):
    """
    æŸ¥è¯¢æŒ‡å®šthread_idçš„å†å²æ¶ˆæ¯
    
    è¿™æ˜¯ä¸‰è§’èŠå¤©å®¤çš„æ ¸å¿ƒAPIï¼Œè¿”å›æ‰€æœ‰sourceï¼ˆuser/api/assistantï¼‰çš„æ¶ˆæ¯
    
    Args:
        thread_id: å¯¹è¯çº¿ç¨‹ID
        limit: æœ€å¤§è¿”å›æ•°é‡
    
    Returns:
        List[Dict]: æ¶ˆæ¯å†å²åˆ—è¡¨
    """
    try:
        import sqlite3
        import json
        from pathlib import Path
        
        db_path = "/data/memory_buffer.db"
        
        # æ£€æŸ¥æ•°æ®åº“æ˜¯å¦å­˜åœ¨
        if not Path(db_path).exists():
            return {"messages": [], "thread_id": thread_id}
        
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()
        
        # æŸ¥è¯¢æŒ‡å®šthread_idçš„dialogueç±»å‹æ¶ˆæ¯
        cursor.execute("""
            SELECT * FROM memory_buffer
            WHERE type = 'dialogue'
            AND json_extract(metadata, '$.thread_id') = ?
            ORDER BY timestamp ASC
            LIMIT ?
        """, (thread_id, limit))
        
        records = cursor.fetchall()
        conn.close()
        
        # æ ¼å¼åŒ–æ¶ˆæ¯
        messages = []
        for record in records:
            metadata = json.loads(record["metadata"]) if record["metadata"] else {}
            messages.append({
                "id": record["id"],
                "timestamp": record["timestamp"],
                "content": record["content"],
                "source": metadata.get("source", "unknown"),
                "interface": metadata.get("interface", "unknown"),
                "metadata": metadata
            })
        
        return {
            "messages": messages,
            "thread_id": thread_id,
            "count": len(messages)
        }
        
    except Exception as e:
        logger.error(f"Failed to get thread history: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to get history: {str(e)}")

# ============================================
# OpenAI Compatible API Endpoints
# ============================================

@app.get("/v1/models")
async def list_models():
    """OpenAI å…¼å®¹ï¼šåˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    try:
        # å°è¯•ä» LM Studio è·å–æ¨¡å‹åˆ—è¡¨
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get(f"{settings.LLM_BASE_URL}/models")
            if response.status_code == 200:
                return response.json()
    except Exception:
        pass
    
    # å›é€€ï¼šè¿”å›é…ç½®çš„æ¨¡å‹
    return OpenAIModelsResponse(
        data=[OpenAIModel(id=settings.LLM_MODEL)]
    ).dict()

@app.post("/v1/chat/completions")
async def chat_completions(request: OpenAIChatRequest):
    """OpenAI å…¼å®¹ï¼šèŠå¤©è¡¥å…¨æ¥å£ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰"""
    try:
        # è½¬æ¢ OpenAI æ ¼å¼çš„æ¶ˆæ¯ä¸º LangChain æ ¼å¼
        langchain_messages = []
        for msg in request.messages:
            if msg.role == "system":
                langchain_messages.append(SystemMessage(content=msg.content))
            elif msg.role == "user":
                langchain_messages.append(HumanMessage(content=msg.content))
            elif msg.role == "assistant":
                langchain_messages.append(AIMessage(content=msg.content or ""))
        
        # è°ƒç”¨ Agent å·¥ä½œæµ (v6.3.2: ä½¿ç”¨app_state)
        config = {"configurable": {"thread_id": f"openai_{int(time.time())}"}}
        result = app_state["app_graph"].invoke(
            {"messages": langchain_messages},
            config=config
        )
        
        # æå–æœ€ç»ˆå“åº”
        messages = result["messages"]
        last_message = messages[-1]
        
        # æ„å»º OpenAI æ ¼å¼çš„å“åº”
        response_message = {
            "role": "assistant",
            "content": last_message.content if hasattr(last_message, "content") else str(last_message)
        }
        
        # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ·»åŠ  tool_calls
        if hasattr(last_message, "tool_calls") and last_message.tool_calls:
            response_message["tool_calls"] = [
                {
                    "id": f"call_{i}",
                    "type": "function",
                    "function": {
                        "name": tc["name"],
                        "arguments": json.dumps(tc["args"])
                    }
                }
                for i, tc in enumerate(last_message.tool_calls)
            ]
        
        # è¿”å› OpenAI æ ¼å¼
        return {
            "id": f"chatcmpl-{int(time.time())}",
            "object": "chat.completion",
            "created": int(time.time()),
            "model": request.model,
            "choices": [
                {
                    "index": 0,
                    "message": response_message,
                    "finish_reason": "tool_calls" if "tool_calls" in response_message else "stop"
                }
            ],
            "usage": {
                "prompt_tokens": 0,
                "completion_tokens": 0,
                "total_tokens": 0
            }
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Chat completion failed: {str(e)}")

@app.post("/v1/completions")
async def completions(request: Dict[str, Any]):
    """OpenAI å…¼å®¹ï¼šæ–‡æœ¬è¡¥å…¨æ¥å£ï¼ˆä»£ç†åˆ° LM Studioï¼‰"""
    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                f"{settings.LLM_BASE_URL}/completions",
                json=request,
                headers={"Content-Type": "application/json"}
            )
            return response.json()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Completion failed: {str(e)}")

@app.post("/v1/embeddings")
async def embeddings(request: Dict[str, Any]):
    """OpenAI å…¼å®¹ï¼šæ–‡æœ¬åµŒå…¥æ¥å£ï¼ˆä»£ç†åˆ° LM Studioï¼‰"""
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(
                f"{settings.LLM_BASE_URL}/embeddings",
                json=request,
                headers={"Content-Type": "application/json"}
            )
            return response.json()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Embeddings failed: {str(e)}")

# ============================================
# WebSocket Chat Room (ä¸‰è§’èŠå¤©å®¤)
# ============================================

@app.websocket("/ws/chat/{thread_id}")
async def websocket_chat(websocket: WebSocket, thread_id: str):
    """
    WebSocket ä¸‰è§’èŠå¤©å®¤æ¥å£
    
    å®¢æˆ·ç«¯è¿æ¥åï¼Œä¼šå®æ—¶æ¥æ”¶è¯¥thread_idçš„æ‰€æœ‰æ¶ˆæ¯ï¼ˆuser/api/assistantï¼‰
    
    Args:
        thread_id: å¯¹è¯çº¿ç¨‹ID
    """
    await ws_manager.connect(websocket, thread_id)
    
    try:
        # å‘é€æ¬¢è¿æ¶ˆæ¯
        await ws_manager.send_personal_message(
            websocket,
            {
                "type": "connected",
                "thread_id": thread_id,
                "message": f"Connected to thread {thread_id}",
                "connections": ws_manager.get_thread_connections_count(thread_id)
            }
        )
        
        # ä¿æŒè¿æ¥ï¼Œç­‰å¾…æ¶ˆæ¯
        while True:
            # æ¥æ”¶å®¢æˆ·ç«¯æ¶ˆæ¯ï¼ˆå¦‚å¿ƒè·³åŒ…ï¼‰
            data = await websocket.receive_text()
            
            # å¤„ç†å¿ƒè·³åŒ…
            if data == "ping":
                await ws_manager.send_personal_message(
                    websocket,
                    {"type": "pong", "timestamp": time.time()}
                )
    
    except WebSocketDisconnect:
        ws_manager.disconnect(websocket, thread_id)
        logger.info(f"WebSocket disconnected from thread {thread_id}")
    
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
        ws_manager.disconnect(websocket, thread_id)

@app.get("/chat-room", response_class=HTMLResponse)
async def chat_room_placeholder():
    """èŠå¤©å®¤å ä½é¡µé¢ï¼ˆæœªæ¥æ›¿æ¢ä¸ºå®Œæ•´çš„å‰ç«¯ï¼‰"""
    return """
    <!DOCTYPE html>
    <html>
    <head>
        <title>M3 Agent èŠå¤©å®¤ï¼ˆå¼€å‘ä¸­ï¼‰</title>
        <style>
            body { font-family: Arial, sans-serif; max-width: 800px; margin: 50px auto; padding: 20px; }
            h1 { color: #667eea; }
            .note { background: #f0f0f0; padding: 15px; border-radius: 8px; }
        </style>
    </head>
    <body>
        <h1>ğŸš€ M3 Agent èˆ°é˜ŸèŠå¤©å®¤</h1>
        <div class="note">
            <p><strong>çŠ¶æ€ï¼š</strong>å¼€å‘ä¸­</p>
            <p><strong>WebSocket ç«¯ç‚¹ï¼š</strong><code>ws://localhost:8001/ws/chat</code></p>
            <p><strong>è®¡åˆ’åŠŸèƒ½ï¼š</strong></p>
            <ul>
                <li>å¤šç”¨æˆ·å®æ—¶èŠå¤©</li>
                <li>Agent å·¥å…·è°ƒç”¨å¯è§†åŒ–</li>
                <li>å¯¹è¯å†å²å›æ”¾</li>
                <li>å¤šæ¨¡æ€æ¶ˆæ¯æ”¯æŒï¼ˆæ–‡æœ¬ã€å›¾ç‰‡ã€ä»£ç ï¼‰</li>
            </ul>
        </div>
    </body>
    </html>
    """

# ============================================
# Mount Admin Panel
# ============================================

# æ³¨æ„ï¼šadmin_app å°†åœ¨å•ç‹¬çš„è¿›ç¨‹ä¸­è¿è¡Œï¼ˆç«¯å£ 8002ï¼‰
# è¿™é‡Œä¸éœ€è¦ mountï¼Œå› ä¸ºå®ƒä»¬æ˜¯ç‹¬ç«‹çš„æœåŠ¡

# ============================================
# Memory Sync Startup
# ============================================

from app.memory.memory_sync import start_memory_sync, stop_memory_sync
from app.core.browser_pool import shutdown_browser_pool
from app.core.tool_pool import tool_pool  # v5.7: Global tool pool
import atexit

# å¯åŠ¨è®°å¿†åŒæ­¥
start_memory_sync()
logger.info("âœ“ Memory sync worker started")

# v5.7: Initialize tool pool (async initialization will be done in startup event)
# Note: Actual loading happens in @app.on_event("startup")
logger.info("âœ“ Tool pool ready for initialization")

# æ³¨å†Œå…³é—­é’©å­
atexit.register(stop_memory_sync)
atexit.register(shutdown_browser_pool)  # v5.0: Shutdown browser pool on exit
atexit.register(lambda: asyncio.run(tool_pool.shutdown()))  # v5.7: Shutdown tool pool

# v5.9: Shutdown background tasks manager
from app.core.background_tasks import background_tasks_manager
atexit.register(lambda: asyncio.run(background_tasks_manager.stop()))

logger.info("âœ“ Shutdown hooks registered (memory sync + browser pool + tool pool + background tasks)")

# ============================================
# Main Entry Point
# ============================================

if __name__ == "__main__":
    import uvicorn
    # v5.7.1: Use default uvloop for performance (browser pool uses thread pool)
    uvicorn.run(
        app,
        host=settings.API_HOST,
        port=settings.API_PORT,
        log_level="info"
        # No loop="asyncio" - let uvicorn use uvloop by default
    )


# ============================================
# ä¸´æ—¶è°ƒè¯•ç«¯ç‚¹ (v6.3.2 debug)
# ============================================

@app.get("/debug/app_state")
async def debug_app_state():
    """è°ƒè¯•: æ£€æŸ¥app_stateçš„å®é™…å†…å®¹"""
    return {
        "app_state_id": id(app_state),
        "browser_pool": app_state["browser_pool"] is not None,
        "tools_count": len(app_state["tools"]),
        "tools_sample": [
            {"name": t.name, "description": t.description[:50]}
            for t in app_state["tools"][:3]
        ] if app_state["tools"] else [],
        "llm_with_tools": app_state["llm_with_tools"] is not None,
        "app_graph": app_state["app_graph"] is not None,
    }
